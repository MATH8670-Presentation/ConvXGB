{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Train xgboost by epoch. \r\n",
    "# Start with default weights, train XGBoost, produce accuracy\r\n",
    "# Run epoch again, but default weight will change, train XGBoost, product accuracy\r\n",
    "# continue"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import keras\r\n",
    "from keras.utils.np_utils import to_categorical\r\n",
    "\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "import xgboost as xgb\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data = pd.read_csv('./DATA/sensorless_data.csv')\r\n",
    "data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>...</th>\n",
       "      <th>F40</th>\n",
       "      <th>F41</th>\n",
       "      <th>F42</th>\n",
       "      <th>F43</th>\n",
       "      <th>F44</th>\n",
       "      <th>F45</th>\n",
       "      <th>F46</th>\n",
       "      <th>F47</th>\n",
       "      <th>F48</th>\n",
       "      <th>TARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.014600e-07</td>\n",
       "      <td>8.260300e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.438600e-06</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>-0.032963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63308</td>\n",
       "      <td>2.9646</td>\n",
       "      <td>8.1198</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.913200e-06</td>\n",
       "      <td>-5.247700e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>2.778900e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59314</td>\n",
       "      <td>7.6252</td>\n",
       "      <td>6.1690</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.951700e-06</td>\n",
       "      <td>-3.184000e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.575300e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63252</td>\n",
       "      <td>2.7784</td>\n",
       "      <td>5.3017</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.322600e-06</td>\n",
       "      <td>8.820100e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-7.282900e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.029417</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.62289</td>\n",
       "      <td>6.5534</td>\n",
       "      <td>6.2606</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4976</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.836600e-08</td>\n",
       "      <td>5.666300e-07</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-7.940600e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.031393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63010</td>\n",
       "      <td>4.5155</td>\n",
       "      <td>9.5231</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             F1            F2        F3        F4            F5        F6  \\\n",
       "0 -3.014600e-07  8.260300e-06 -0.000012 -0.000002 -1.438600e-06 -0.000021   \n",
       "1  2.913200e-06 -5.247700e-06  0.000003 -0.000006  2.778900e-06 -0.000004   \n",
       "2 -2.951700e-06 -3.184000e-06 -0.000016 -0.000001 -1.575300e-06  0.000017   \n",
       "3 -1.322600e-06  8.820100e-06 -0.000016 -0.000005 -7.282900e-07  0.000004   \n",
       "4 -6.836600e-08  5.666300e-07 -0.000026 -0.000006 -7.940600e-07  0.000013   \n",
       "\n",
       "         F7        F8        F9       F10  ...      F40     F41     F42  \\\n",
       "0  0.031718  0.031710  0.031721 -0.032963  ... -0.63308  2.9646  8.1198   \n",
       "1  0.030804  0.030810  0.030806 -0.033520  ... -0.59314  7.6252  6.1690   \n",
       "2  0.032877  0.032880  0.032896 -0.029834  ... -0.63252  2.7784  5.3017   \n",
       "3  0.029410  0.029401  0.029417 -0.030156  ... -0.62289  6.5534  6.2606   \n",
       "4  0.030119  0.030119  0.030145 -0.031393  ... -0.63010  4.5155  9.5231   \n",
       "\n",
       "      F43     F44     F45     F46     F47     F48  TARGET  \n",
       "0 -1.4961 -1.4961 -1.4961 -1.4996 -1.4996 -1.4996       1  \n",
       "1 -1.4967 -1.4967 -1.4967 -1.5005 -1.5005 -1.5005       1  \n",
       "2 -1.4983 -1.4983 -1.4982 -1.4985 -1.4985 -1.4985       1  \n",
       "3 -1.4963 -1.4963 -1.4963 -1.4975 -1.4975 -1.4976       1  \n",
       "4 -1.4958 -1.4958 -1.4958 -1.4959 -1.4959 -1.4959       1  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Format input data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "x_cols = list(data.columns[:-1])\r\n",
    "X_data = data[x_cols].copy()\r\n",
    "# Adding 0 for easy reshaping\r\n",
    "X_data['F49'] = 0\r\n",
    "X_data.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>F10</th>\n",
       "      <th>...</th>\n",
       "      <th>F40</th>\n",
       "      <th>F41</th>\n",
       "      <th>F42</th>\n",
       "      <th>F43</th>\n",
       "      <th>F44</th>\n",
       "      <th>F45</th>\n",
       "      <th>F46</th>\n",
       "      <th>F47</th>\n",
       "      <th>F48</th>\n",
       "      <th>F49</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.014600e-07</td>\n",
       "      <td>8.260300e-06</td>\n",
       "      <td>-0.000012</td>\n",
       "      <td>-0.000002</td>\n",
       "      <td>-1.438600e-06</td>\n",
       "      <td>-0.000021</td>\n",
       "      <td>0.031718</td>\n",
       "      <td>0.031710</td>\n",
       "      <td>0.031721</td>\n",
       "      <td>-0.032963</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63308</td>\n",
       "      <td>2.9646</td>\n",
       "      <td>8.1198</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4961</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>-1.4996</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.913200e-06</td>\n",
       "      <td>-5.247700e-06</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>2.778900e-06</td>\n",
       "      <td>-0.000004</td>\n",
       "      <td>0.030804</td>\n",
       "      <td>0.030810</td>\n",
       "      <td>0.030806</td>\n",
       "      <td>-0.033520</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.59314</td>\n",
       "      <td>7.6252</td>\n",
       "      <td>6.1690</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.4967</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>-1.5005</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.951700e-06</td>\n",
       "      <td>-3.184000e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000001</td>\n",
       "      <td>-1.575300e-06</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.032877</td>\n",
       "      <td>0.032880</td>\n",
       "      <td>0.032896</td>\n",
       "      <td>-0.029834</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63252</td>\n",
       "      <td>2.7784</td>\n",
       "      <td>5.3017</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4983</td>\n",
       "      <td>-1.4982</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>-1.4985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.322600e-06</td>\n",
       "      <td>8.820100e-06</td>\n",
       "      <td>-0.000016</td>\n",
       "      <td>-0.000005</td>\n",
       "      <td>-7.282900e-07</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.029410</td>\n",
       "      <td>0.029401</td>\n",
       "      <td>0.029417</td>\n",
       "      <td>-0.030156</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.62289</td>\n",
       "      <td>6.5534</td>\n",
       "      <td>6.2606</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4963</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4975</td>\n",
       "      <td>-1.4976</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-6.836600e-08</td>\n",
       "      <td>5.666300e-07</td>\n",
       "      <td>-0.000026</td>\n",
       "      <td>-0.000006</td>\n",
       "      <td>-7.940600e-07</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030119</td>\n",
       "      <td>0.030145</td>\n",
       "      <td>-0.031393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.63010</td>\n",
       "      <td>4.5155</td>\n",
       "      <td>9.5231</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4958</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>-1.4959</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             F1            F2        F3        F4            F5        F6  \\\n",
       "0 -3.014600e-07  8.260300e-06 -0.000012 -0.000002 -1.438600e-06 -0.000021   \n",
       "1  2.913200e-06 -5.247700e-06  0.000003 -0.000006  2.778900e-06 -0.000004   \n",
       "2 -2.951700e-06 -3.184000e-06 -0.000016 -0.000001 -1.575300e-06  0.000017   \n",
       "3 -1.322600e-06  8.820100e-06 -0.000016 -0.000005 -7.282900e-07  0.000004   \n",
       "4 -6.836600e-08  5.666300e-07 -0.000026 -0.000006 -7.940600e-07  0.000013   \n",
       "\n",
       "         F7        F8        F9       F10  ...      F40     F41     F42  \\\n",
       "0  0.031718  0.031710  0.031721 -0.032963  ... -0.63308  2.9646  8.1198   \n",
       "1  0.030804  0.030810  0.030806 -0.033520  ... -0.59314  7.6252  6.1690   \n",
       "2  0.032877  0.032880  0.032896 -0.029834  ... -0.63252  2.7784  5.3017   \n",
       "3  0.029410  0.029401  0.029417 -0.030156  ... -0.62289  6.5534  6.2606   \n",
       "4  0.030119  0.030119  0.030145 -0.031393  ... -0.63010  4.5155  9.5231   \n",
       "\n",
       "      F43     F44     F45     F46     F47     F48  F49  \n",
       "0 -1.4961 -1.4961 -1.4961 -1.4996 -1.4996 -1.4996    0  \n",
       "1 -1.4967 -1.4967 -1.4967 -1.5005 -1.5005 -1.5005    0  \n",
       "2 -1.4983 -1.4983 -1.4982 -1.4985 -1.4985 -1.4985    0  \n",
       "3 -1.4963 -1.4963 -1.4963 -1.4975 -1.4975 -1.4976    0  \n",
       "4 -1.4958 -1.4958 -1.4958 -1.4959 -1.4959 -1.4959    0  \n",
       "\n",
       "[5 rows x 49 columns]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "len(X_data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "58509"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X = X_data.to_numpy().reshape((len(X_data),7,7,1))\r\n",
    "X.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(58509, 7, 7, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X[0].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7, 7, 1)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "data['TARGET'] = data['TARGET'] - 1\r\n",
    "y = data['TARGET'].to_numpy()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "y = to_categorical(y, num_classes=data['TARGET'].nunique())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Randomizing dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "idxs = np.arange(len(X))\r\n",
    "samples = np.random.choice(idxs,size=10000)\r\n",
    "\r\n",
    "X_rand = X[samples]\r\n",
    "y_rand = y[samples]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CNN Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# Only contains layers up to flatten\r\n",
    "# Xgboost will take us to final 11 logits (probailities)\r\n",
    "model = keras.models.Sequential([\r\n",
    "    keras.layers.Conv2D(256, (2,2), strides=1, activation='relu', input_shape=(7,7,1)),\r\n",
    "    keras.layers.Conv2D(256, (2,2), strides=1, activation='relu'),\r\n",
    "    keras.layers.Flatten()\r\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "model.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 6, 6, 256)         1280      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 256)         262400    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6400)              0         \n",
      "=================================================================\n",
      "Total params: 263,680\n",
      "Trainable params: 263,680\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Combined model attempt"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "optimizer = keras.optimizers.Adam()\r\n",
    "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "params = {'objective':'multi:softprob', 'num_class':11}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_rand[:10000], y_rand[:10000]))\r\n",
    "train_dataset = train_dataset.batch(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "xs = X_rand[:10000]\r\n",
    "ys = y_rand[:10000]\r\n",
    "features = model(xs, training=True)\r\n",
    "labels = [list(r).index(0) for r in ys]\r\n",
    "\r\n",
    "dtrain = xgb.DMatrix(features.numpy(), label=labels)\r\n",
    "bst = xgb.train(params, dtrain)\r\n",
    "logits = bst.predict(dtrain)\r\n",
    "\r\n",
    "tf_xs = tf.convert_to_tensor(xs)\r\n",
    "tf_ys = tf.convert_to_tensor(ys)\r\n",
    "tf_logits = tf.convert_to_tensor(logits)\r\n",
    "with tf.GradientTape() as tape:\r\n",
    "    tape.watch(model.trainable_weights)\r\n",
    "    # Run the forward pass of the layer.\r\n",
    "    # The operations that the layer applies\r\n",
    "    # to its inputs are going to be recorded\r\n",
    "    # on the GradientTape\r\n",
    "#     features = model(xs, training=True)  # Logits for this minibatch\r\n",
    "\r\n",
    "    # Compute the loss value for this minibatch.\r\n",
    "    loss_value = loss_fn(tf_ys, tf_logits)\r\n",
    "\r\n",
    "# Use the gradient tape to automatically retrieve\r\n",
    "# the gradients of the trainable variables with respect to the loss.\r\n",
    "grads = tape.gradient(loss_value, model.trainable_weights)\r\n",
    "\r\n",
    "# Run one step of gradient descent by updating\r\n",
    "# the value of the variables to minimize the loss.\r\n",
    "optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[22:00:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0'].",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-0c652ee2b253>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m# Run one step of gradient descent by updating\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# the value of the variables to minimize the loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36mapply_gradients\u001b[1;34m(self, grads_and_vars, name, experimental_aggregate_gradients)\u001b[0m\n\u001b[0;32m    511\u001b[0m       \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mnone\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mhave\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m     \"\"\"\n\u001b[1;32m--> 513\u001b[1;33m     \u001b[0mgrads_and_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_filter_grads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\optimizer_v2\\optimizer_v2.py\u001b[0m in \u001b[0;36m_filter_grads\u001b[1;34m(grads_and_vars)\u001b[0m\n\u001b[0;32m   1268\u001b[0m   \u001b[0mfiltered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfiltered\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m     raise ValueError(\"No gradients provided for any variable: %s.\" %\n\u001b[0m\u001b[0;32m   1271\u001b[0m                      ([v.name for _, v in grads_and_vars],))\n\u001b[0;32m   1272\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mvars_with_empty_grads\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: No gradients provided for any variable: ['conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0']."
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "tape.gradient?"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0msources\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0munconnected_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m<\u001b[0m\u001b[0mUnconnectedGradients\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNONE\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'none'\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Computes the gradient using operations recorded in context of this tape.\n",
       "\n",
       "Args:\n",
       "  target: a list or nested structure of Tensors or Variables to be\n",
       "    differentiated.\n",
       "  sources: a list or nested structure of Tensors or Variables. `target`\n",
       "    will be differentiated against elements in `sources`.\n",
       "  output_gradients: a list of gradients, one for each element of\n",
       "    target. Defaults to None.\n",
       "  unconnected_gradients: a value which can either hold 'none' or 'zero' and\n",
       "    alters the value which will be returned if the target and sources are\n",
       "    unconnected. The possible values and effects are detailed in\n",
       "    'UnconnectedGradients' and it defaults to 'none'.\n",
       "\n",
       "Returns:\n",
       "  a list or nested structure of Tensors (or IndexedSlices, or None),\n",
       "  one for each element in `sources`. Returned structure is the same as\n",
       "  the structure of `sources`.\n",
       "\n",
       "Raises:\n",
       "  RuntimeError: if called inside the context of the tape, or if called more\n",
       "   than once on a non-persistent tape.\n",
       "  ValueError: if the target is a variable or if unconnected gradients is\n",
       "   called with an unknown value.\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\users\\thejj\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\n",
       "\u001b[1;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "source": [
    "# for (x_batch_train, y_batch_train) in train_dataset:\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         # Run the forward pass of the layer.\n",
    "#         # The operations that the layer applies\n",
    "#         # to its inputs are going to be recorded\n",
    "#         # on the GradientTape\n",
    "#         features = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "#         labels = [list(r).index(0) for r in y_batch_train]\n",
    "\n",
    "#         dtrain = xgb.DMatrix(features.numpy(), label=labels)\n",
    "#         bst = xgb.train(params, dtrain)\n",
    "#         logits = tf.convert_to_tensor(bst.predict(dtrain))\n",
    "\n",
    "#         # Compute the loss value for this minibatch.\n",
    "#         loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "#     # Use the gradient tape to automatically retrieve\n",
    "#     # the gradients of the trainable variables with respect to the loss.\n",
    "#     grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "#     # Run one step of gradient descent by updating\n",
    "#     # the value of the variables to minimize the loss.\n",
    "#     optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "source": [
    "tape.watched_variables()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Variable 'conv2d_2/kernel:0' shape=(2, 2, 1, 256) dtype=float32, numpy=\n",
       " array([[[[-0.07284722,  0.04625756, -0.03468293, ...,  0.04178733,\n",
       "           -0.05144799,  0.04714624]],\n",
       " \n",
       "         [[-0.04565594, -0.00010972, -0.0097011 , ...,  0.05431872,\n",
       "            0.01638418,  0.0102551 ]]],\n",
       " \n",
       " \n",
       "        [[[-0.04973967,  0.06152815,  0.05775816, ..., -0.06755853,\n",
       "            0.04159236,  0.01956882]],\n",
       " \n",
       "         [[ 0.04163671,  0.05185356,  0.03649052, ...,  0.03251676,\n",
       "            0.0119964 , -0.00093066]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_2/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_3/kernel:0' shape=(2, 2, 256, 256) dtype=float32, numpy=\n",
       " array([[[[ 0.03082415,  0.00809135,  0.04507285, ...,  0.00319395,\n",
       "            0.03115386,  0.05264703],\n",
       "          [ 0.04060483, -0.04027739, -0.04701884, ..., -0.04393014,\n",
       "            0.00499388,  0.01641734],\n",
       "          [ 0.02991765,  0.01192734,  0.0373666 , ..., -0.03394721,\n",
       "           -0.01118625,  0.0376766 ],\n",
       "          ...,\n",
       "          [ 0.00656231, -0.03762408, -0.02217661, ..., -0.0128004 ,\n",
       "           -0.00580621,  0.02482142],\n",
       "          [-0.01233191, -0.02359189, -0.00560464, ..., -0.04362156,\n",
       "            0.02867949,  0.03663956],\n",
       "          [-0.02350583, -0.00814705, -0.04396211, ...,  0.03362952,\n",
       "            0.01998353,  0.00803915]],\n",
       " \n",
       "         [[ 0.02482189, -0.04938278,  0.04691071, ...,  0.04604121,\n",
       "           -0.02296035,  0.01369056],\n",
       "          [-0.00069101, -0.04584108, -0.00147071, ...,  0.01406493,\n",
       "           -0.01050776, -0.03379419],\n",
       "          [-0.0390796 ,  0.03211683,  0.00363685, ...,  0.04878708,\n",
       "           -0.02431723, -0.02013512],\n",
       "          ...,\n",
       "          [-0.00941976,  0.03237137,  0.02648949, ...,  0.00853078,\n",
       "           -0.0228349 ,  0.02222377],\n",
       "          [ 0.00274212, -0.03590623, -0.02860409, ...,  0.04678601,\n",
       "           -0.00297118,  0.01488692],\n",
       "          [-0.01053759, -0.03265056, -0.04190134, ...,  0.0208296 ,\n",
       "           -0.03435509,  0.05078436]]],\n",
       " \n",
       " \n",
       "        [[[ 0.00224159, -0.02223177,  0.05252284, ...,  0.04218778,\n",
       "           -0.05194776,  0.0479431 ],\n",
       "          [ 0.05089377, -0.00080139, -0.0416879 , ..., -0.0345723 ,\n",
       "           -0.01096583, -0.04837198],\n",
       "          [ 0.03513266, -0.00603883,  0.04252019, ...,  0.00317365,\n",
       "           -0.02280962,  0.00767955],\n",
       "          ...,\n",
       "          [-0.02381615,  0.04656674,  0.03919714, ..., -0.04753849,\n",
       "           -0.02518637,  0.03615512],\n",
       "          [-0.01887371,  0.03961264, -0.03079174, ..., -0.00382068,\n",
       "            0.02179721, -0.04752022],\n",
       "          [-0.02278048, -0.01502912,  0.01594058, ..., -0.03410653,\n",
       "           -0.04399127,  0.036049  ]],\n",
       " \n",
       "         [[-0.01309594,  0.04931014,  0.05306238, ..., -0.00644755,\n",
       "           -0.05134465,  0.01625106],\n",
       "          [ 0.0306887 ,  0.00203854,  0.05115723, ..., -0.00033261,\n",
       "           -0.05242961, -0.0358076 ],\n",
       "          [-0.02800141,  0.03968407,  0.02253535, ...,  0.03159902,\n",
       "           -0.03306367,  0.02216656],\n",
       "          ...,\n",
       "          [ 0.04240578, -0.00201268,  0.04425728, ...,  0.02960657,\n",
       "            0.0294335 , -0.04436914],\n",
       "          [-0.00679233, -0.03891618, -0.011391  , ..., -0.02527959,\n",
       "            0.01939153, -0.0305223 ],\n",
       "          [-0.01542043, -0.05193375, -0.00987975, ..., -0.02018768,\n",
       "           -0.05025713, -0.01184414]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_3/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0.], dtype=float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 92
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [
    "logits"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.94750625, 0.00535458, 0.00523769, ..., 0.00523769, 0.00523769,\n",
       "        0.00523769],\n",
       "       [0.9476097 , 0.00524588, 0.00523827, ..., 0.00523827, 0.00523827,\n",
       "        0.00523827],\n",
       "       [0.9476097 , 0.00524588, 0.00523827, ..., 0.00523827, 0.00523827,\n",
       "        0.00523827],\n",
       "       ...,\n",
       "       [0.9476097 , 0.00524588, 0.00523827, ..., 0.00523827, 0.00523827,\n",
       "        0.00523827],\n",
       "       [0.94750625, 0.00535458, 0.00523769, ..., 0.00523769, 0.00523769,\n",
       "        0.00523769],\n",
       "       [0.9476097 , 0.00524588, 0.00523827, ..., 0.00523827, 0.00523827,\n",
       "        0.00523827]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "source": [
    "loss_value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.5291567>"
      ]
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "y_rand[:10000].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "\n",
    "        # Open a GradientTape to record the operations run\n",
    "        # during the forward pass, which enables auto-differentiation.\n",
    "        with tf.GradientTape() as tape:\n",
    "\n",
    "            # Run the forward pass of the layer.\n",
    "            # The operations that the layer applies\n",
    "            # to its inputs are going to be recorded\n",
    "            # on the GradientTape.\n",
    "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "            # Compute the loss value for this minibatch.\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "        # Use the gradient tape to automatically retrieve\n",
    "        # the gradients of the trainable variables with respect to the loss.\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "        # Run one step of gradient descent by updating\n",
    "        # the value of the variables to minimize the loss.\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # Log every 200 batches.\n",
    "        if step % 200 == 0:\n",
    "            print(\n",
    "                \"Training loss (for one batch) at step %d: %.4f\"\n",
    "                % (step, float(loss_value))\n",
    "            )\n",
    "            print(\"Seen so far: %s samples\" % ((step + 1) * 64))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}